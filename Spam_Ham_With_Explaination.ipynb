{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e2ebc9618fb5f5407b11492d3d8b4a02d39e77fbbc3b477930cec636582b7fc5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Let's start by opening the MailSpamCollection file with the read_csv() function from the pandas package."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mail_spam = pd.read_csv('dataset2', sep='\\t',header=None, names=['Label', 'MAIL'])\n",
    "\n",
    "print(mail_spam.shape)\n",
    "mail_spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mail_spam['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "source": [
    "We're now going to split our dataset into a training set and a test set. We'll use 80% of the data for training and the remaining 20% for testing.\n",
    "  \n",
    "We'll randomize the entire dataset before splitting to ensure that spam and ham messages are spread properly throughout the dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the dataset\n",
    "data_randomized = mail_spam.sample(frac=1, random_state=1)\n",
    "\n",
    "# Calculate index for split\n",
    "training_test_index = round(len(data_randomized) * 0.8)\n",
    "\n",
    "# Split into training and test sets\n",
    "training_set = data_randomized[:training_test_index].reset_index(drop=True)\n",
    "test_set = data_randomized[training_test_index:].reset_index(drop=True)\n",
    "\n",
    "print(training_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "source": [
    "Let's begin the data cleaning process by removing the punctuation and making all the words lowercase."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before cleaning\n",
    "training_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cleaning\n",
    "training_set['MAIL'] = training_set['MAIL'].str.replace('\\W', ' ') # Removes punctuation\n",
    "training_set['MAIL'] = training_set['MAIL'].str.lower()\n",
    "training_set.head(3)"
   ]
  },
  {
   "source": [
    "Let's now create the vocabulary, which in this context means a list with all the unique words in our training set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['MAIL'] = training_set['MAIL'].str.split()\n",
    "\n",
    "vocabulary = []\n",
    "for mail in training_set['MAIL']:\n",
    "   for word in mail:\n",
    "      vocabulary.append(word)\n",
    "\n",
    "vocabulary = list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "source": [
    "Creating clean training data set.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_per_mail = {unique_word: [0] * len(training_set['MAIL']) for unique_word in vocabulary}\n",
    "\n",
    "for index, mail in enumerate(training_set['MAIL']):\n",
    "   for word in mail:\n",
    "      word_counts_per_mail[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = pd.DataFrame(word_counts_per_mail)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_clean = pd.concat([training_set, word_counts], axis=1)\n",
    "training_set_clean.head()"
   ]
  },
  {
   "source": [
    "Calculating Constants First"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating spam and ham messages first\n",
    "spam_messages = training_set_clean[training_set_clean['Label'] == 'spam']\n",
    "ham_messages = training_set_clean[training_set_clean['Label'] == 'ham']\n",
    "\n",
    "# P(Spam) and P(Ham)\n",
    "p_spam = len(spam_messages) / len(training_set_clean)\n",
    "p_ham = len(ham_messages) / len(training_set_clean)\n",
    "\n",
    "# N_Spam\n",
    "n_words_per_spam_message = spam_messages['MAIL'].apply(len)\n",
    "n_spam = n_words_per_spam_message.sum()\n",
    "\n",
    "# N_Ham\n",
    "n_words_per_ham_message = ham_messages['MAIL'].apply(len)\n",
    "n_ham = n_words_per_ham_message.sum()\n",
    "\n",
    "# N_Vocabulary\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1"
   ]
  },
  {
   "source": [
    "Calculating Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate parameters\n",
    "parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "# Calculate parameters\n",
    "for word in vocabulary:\n",
    "   n_word_given_spam = spam_messages[word].sum() # spam_messages already defined\n",
    "   p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "   parameters_spam[word] = p_word_given_spam\n",
    "\n",
    "   n_word_given_ham = ham_messages[word].sum() # ham_messages already defined\n",
    "   p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "   parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "source": [
    "Creating a function to Classifying A New Message"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def classify(message):\n",
    "\n",
    "   message = re.sub('\\W', ' ', message)\n",
    "   message = message.lower().split()\n",
    "\n",
    "   p_spam_given_message = p_spam\n",
    "   p_ham_given_message = p_ham\n",
    "\n",
    "   for word in message:\n",
    "      if word in parameters_spam:\n",
    "         p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "      if word in parameters_ham: \n",
    "         p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "   print('P(Spam|message):', p_spam_given_message)\n",
    "   print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "   if p_ham_given_message > p_spam_given_message:\n",
    "      print('Label: Ham')\n",
    "   elif p_ham_given_message < p_spam_given_message:\n",
    "      print('Label: Spam')\n",
    "   else:\n",
    "      print('Equal proabilities, have a human classify this!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify('WINNER!! This is the secret code to unlock the money: C3421.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(\"Sounds good, Tom, then see u there\")"
   ]
  },
  {
   "source": [
    "Measuring the Spam Filter's Accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "\n",
    "   message = re.sub('\\W', ' ', message)\n",
    "   message = message.lower().split()\n",
    "\n",
    "   p_spam_given_message = p_spam\n",
    "   p_ham_given_message = p_ham\n",
    "\n",
    "   for word in message:\n",
    "      if word in parameters_spam:\n",
    "         p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "      if word in parameters_ham:\n",
    "         p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "   if p_ham_given_message > p_spam_given_message:\n",
    "      return 'ham'\n",
    "   elif p_spam_given_message > p_ham_given_message:\n",
    "      return 'spam'\n",
    "   else:\n",
    "      return 'needs human classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['predicted'] = test_set['MAIL'].apply(classify_test_set)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = test_set.shape[0]\n",
    "\n",
    "for row in test_set.iterrows():\n",
    "   row = row[1]\n",
    "   if row['Label'] == row['predicted']:\n",
    "      correct += 1\n",
    "\n",
    "print('Correct:', correct)\n",
    "print('Incorrect:', total - correct)\n",
    "print('Accuracy:', correct/total)"
   ]
  },
  {
   "source": [
    "Fetching Mails from user inbox."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyimap as e\n",
    "import getpass\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "mail_bodies = []\n",
    "password = getpass.getpass(\"Enter the password for your email: \")\n",
    "server = e.connect(\"imap.gmail.com\",\"your email\",password)\n",
    "for i in range(0,len(server.listids())):\n",
    "    email = server.mail(server.listids()[i])\n",
    "    cleantext = BeautifulSoup(email.body, \"lxml\").text\n",
    "    mail_bodies.append(cleantext)\n",
    "server.quit()"
   ]
  },
  {
   "source": [
    "Classifing each mail into Spam/Ham"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for body in mail_bodies:\n",
    "    print(\"<-----------------------------------------------START----------------------------------------------->\\n\")\n",
    "    print(body + \"\\n\\t\\t\\t\\t\\t::::Result::::\")\n",
    "    classify(body)\n",
    "    print(\"<------------------------------------------------END------------------------------------------------>\\n\")"
   ]
  }
 ]
}